{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Improved Malware Detection Model (v2)\n",
    "\n",
    "**Improvements over v1 (ember_malware_detection_clean.ipynb):**\n",
    "1. **Full training data** - Uses all 800k labeled samples instead of 300k\n",
    "2. **No PCA** - Keeps all 2,381 features (PCA was losing detection-critical info)\n",
    "3. **Better hyperparameters** - More trees, regularization, early stopping\n",
    "4. **LightGBM option** - Faster training, often better performance\n",
    "\n",
    "**Expected Results:**\n",
    "- EMBER test accuracy: 95-98%\n",
    "- Training time: ~45-60 min on CPU, ~15 min on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install joblib numpy pandas matplotlib seaborn scikit-learn xgboost lightgbm lief git+https://github.com/elastic/ember.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "import ember\n",
    "import ember.features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, roc_curve, classification_report\n",
    ")\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Monkey patch for ember compatibility\n",
    "def fixed_section_info_process_raw_features(self, raw_obj):\n",
    "    sections = raw_obj['sections']\n",
    "    general = [\n",
    "        len(sections),\n",
    "        sum(1 for s in sections if s['size'] == 0),\n",
    "        sum(1 for s in sections if s['name'] == \"\"),\n",
    "        sum(1 for s in sections if 'MEM_READ' in s['props'] and 'MEM_EXECUTE' in s['props']),\n",
    "        sum(1 for s in sections if 'MEM_WRITE' in s['props'])\n",
    "    ]\n",
    "    section_sizes = [(s['name'], s['size']) for s in sections]\n",
    "    section_sizes_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_sizes]).toarray()[0]\n",
    "    section_entropy = [(s['name'], s['entropy']) for s in sections]\n",
    "    section_entropy_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_entropy]).toarray()[0]\n",
    "    section_vsize = [(s['name'], s['vsize']) for s in sections]\n",
    "    section_vsize_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_vsize]).toarray()[0]\n",
    "    entry_name_hashed = FeatureHasher(50, input_type=\"string\").transform([[raw_obj['entry']]]).toarray()[0]\n",
    "    characteristics = [p for s in sections for p in s['props'] if s['name'] == raw_obj['entry']]\n",
    "    characteristics_hashed = FeatureHasher(50, input_type=\"string\").transform([characteristics]).toarray()[0]\n",
    "    return np.hstack([general, section_sizes_hashed, section_entropy_hashed, section_vsize_hashed, entry_name_hashed, characteristics_hashed]).astype(np.float32)\n",
    "\n",
    "if hasattr(ember.features, 'SectionInfo'):\n",
    "    ember.features.SectionInfo.process_raw_features = fixed_section_info_process_raw_features\n",
    "    print(\"Monkey patch applied.\")\n",
    "\n",
    "# Config\n",
    "DATA_DIR = Path(\"./ember_data\")\n",
    "RESULTS_DIR = Path(\"./results_improved\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and setup EMBER dataset (same as v1)\n",
    "def setup_ember_dataset():\n",
    "    if not DATA_DIR.exists():\n",
    "        DATA_DIR.mkdir(parents=True)\n",
    "    \n",
    "    if not (DATA_DIR / \"train_features.jsonl\").exists() and not (DATA_DIR / \"y_train.dat\").exists():\n",
    "        print(\"Downloading EMBER 2018 dataset...\")\n",
    "        if not os.path.exists(\"ember_dataset_2018_2.tar.bz2\"):\n",
    "            !wget https://ember.elastic.co/ember_dataset_2018_2.tar.bz2\n",
    "        print(\"Extracting...\")\n",
    "        !tar -xvf ember_dataset_2018_2.tar.bz2\n",
    "        source_dir = Path(\"ember2018\")\n",
    "        if source_dir.exists():\n",
    "            for file_path in source_dir.iterdir():\n",
    "                shutil.move(str(file_path), str(DATA_DIR / file_path.name))\n",
    "            shutil.rmtree(str(source_dir))\n",
    "        if Path(\"ember_dataset_2018_2.tar.bz2\").exists():\n",
    "            os.remove(\"ember_dataset_2018_2.tar.bz2\")\n",
    "    else:\n",
    "        print(\"Dataset present.\")\n",
    "    \n",
    "    required_files = [\"X_train.dat\", \"y_train.dat\", \"X_test.dat\", \"y_test.dat\"]\n",
    "    if not all((DATA_DIR / f).exists() for f in required_files):\n",
    "        print(\"Vectorizing features...\")\n",
    "        ember.create_vectorized_features(str(DATA_DIR))\n",
    "    else:\n",
    "        print(\"Vectorized features exist.\")\n",
    "\n",
    "setup_ember_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ALL training data (no subsampling!)\n",
    "print(\"Loading EMBER data (full dataset)...\")\n",
    "\n",
    "X_train, y_train = ember.read_vectorized_features(str(DATA_DIR), subset=\"train\")\n",
    "X_test, y_test = ember.read_vectorized_features(str(DATA_DIR), subset=\"test\")\n",
    "\n",
    "# Filter unlabeled (y != -1)\n",
    "train_mask = y_train != -1\n",
    "test_mask = y_test != -1\n",
    "\n",
    "X_train = X_train[train_mask]\n",
    "y_train = y_train[train_mask]\n",
    "X_test = X_test[test_mask]\n",
    "y_test = y_test[test_mask]\n",
    "\n",
    "# Handle NaNs\n",
    "X_train = np.nan_to_num(X_train, nan=0.0)\n",
    "X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "\n",
    "print(f\"Training samples: {len(y_train):,}\")\n",
    "print(f\"Test samples: {len(y_test):,}\")\n",
    "print(f\"Features: {X_train.shape[1]:,}\")\n",
    "print(f\"Class distribution: {dict(zip(*np.unique(y_train, return_counts=True)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scale_only",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (NO PCA - keep all features!)\n",
    "print(\"Scaling features (keeping all 2,381 features)...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# NO PCA! This is the key improvement\n",
    "X_train_final = X_train_scaled\n",
    "X_test_final = X_test_scaled\n",
    "\n",
    "print(f\"Final training shape: {X_train_final.shape}\")\n",
    "print(f\"Final test shape: {X_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_choice",
   "metadata": {},
   "source": [
    "## Model Training Options\n",
    "\n",
    "Choose ONE of the following cells to run:\n",
    "- **Option A: XGBoost** - More accurate, slower\n",
    "- **Option B: LightGBM** - Faster, similar accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_xgboost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION A: XGBoost with improved hyperparameters\n",
    "print(\"Training XGBoost (improved params)...\")\n",
    "\n",
    "# Split some training data for early stopping validation\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_final, y_train, test_size=0.1, random_state=RANDOM_SEED, stratify=y_train\n",
    ")\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=1500,           # More trees\n",
    "    max_depth=12,                # Slightly less depth (reduce overfit)\n",
    "    learning_rate=0.03,          # Lower LR with more trees\n",
    "    subsample=0.8,               # Row sampling\n",
    "    colsample_bytree=0.8,        # Feature sampling\n",
    "    min_child_weight=5,          # Regularization\n",
    "    gamma=0.1,                   # Min loss reduction\n",
    "    reg_alpha=0.1,               # L1 regularization\n",
    "    reg_lambda=1.0,              # L2 regularization\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    early_stopping_rounds=50,    # Stop if no improvement\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    "    tree_method='hist'           # Use 'gpu_hist' if GPU available\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=100\n",
    ")\n",
    "train_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {train_time/60:.1f} minutes.\")\n",
    "print(f\"Best iteration: {model.best_iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_lightgbm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION B: LightGBM (faster alternative)\n",
    "print(\"Training LightGBM...\")\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_final, y_train, test_size=0.1, random_state=RANDOM_SEED, stratify=y_train\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "    n_estimators=2000,\n",
    "    max_depth=15,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=256,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_samples=20,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    ")\n",
    "train_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {train_time/60:.1f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on EMBER test set\n",
    "print(\"Evaluating on EMBER test set...\")\n",
    "\n",
    "y_pred = model.predict(X_test_final)\n",
    "y_pred_proba = model.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EMBER TEST SET RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {acc:.4f} ({acc*100:.2f}%)\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {auc:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Detailed report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Benign', 'Malicious']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Benign', 'Malicious'], yticklabels=['Benign', 'Malicious'])\n",
    "axes[0].set_title(f'Confusion Matrix (Accuracy: {acc*100:.1f}%)')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "axes[1].plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {auc:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "axes[1].fill_between(fpr, tpr, alpha=0.3)\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curve')\n",
    "axes[1].legend(loc='lower right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'evaluation_plots_improved.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save improved model (to separate directory)\n",
    "print(\"Saving model to ./results_improved/\")\n",
    "\n",
    "# Determine model type\n",
    "model_type = \"XGBoost\" if isinstance(model, xgb.XGBClassifier) else \"LightGBM\"\n",
    "\n",
    "joblib.dump(model, RESULTS_DIR / 'model_improved.pkl')\n",
    "joblib.dump(scaler, RESULTS_DIR / 'scaler_improved.pkl')\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    \"model_type\": model_type,\n",
    "    \"feature_set\": \"EMBER 2018\",\n",
    "    \"preprocessing\": \"StandardScaler (NO PCA)\",\n",
    "    \"n_features\": X_train.shape[1],\n",
    "    \"training_samples\": len(y_train),\n",
    "    \"test_accuracy\": float(acc),\n",
    "    \"test_precision\": float(prec),\n",
    "    \"test_recall\": float(rec),\n",
    "    \"test_f1\": float(f1),\n",
    "    \"test_auc\": float(auc),\n",
    "    \"training_time_minutes\": train_time / 60,\n",
    "    \"improvements_over_v1\": [\n",
    "        \"Full training data (800k vs 300k)\",\n",
    "        \"No PCA (all 2381 features)\",\n",
    "        \"Better hyperparameters\",\n",
    "        \"Early stopping\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / \"experiment_params_improved.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "print(f\"\\nModel saved! Results in {RESULTS_DIR}/\")\n",
    "print(f\"\\nTo use this model in the backend, update main.py to load from 'results_improved/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "## Comparison: v1 vs v2\n",
    "\n",
    "| Aspect | v1 (clean notebook) | v2 (improved) |\n",
    "|--------|---------------------|---------------|\n",
    "| Training samples | 300,000 | 800,000 |\n",
    "| Features used | 1,312 (PCA) | 2,381 (all) |\n",
    "| Expected accuracy | ~93% | ~96-98% |\n",
    "| Training time | ~32 min | ~45-60 min |\n",
    "| Model size | ~40 MB | ~60-80 MB |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
