{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Malware Detection Model Evaluation Framework\n",
    "\n",
    "**Purpose:** Standardized evaluation of malware detection models across multiple industry-grade datasets to measure generalizability and zero-day detection capability.\n",
    "\n",
    "## Evaluation Datasets\n",
    "\n",
    "| Dataset | Type | Samples | Time Period | Purpose |\n",
    "|---------|------|---------|-------------|--------|\n",
    "| EMBER 2018 Test | Benchmark | 200k | 2018 | In-distribution baseline |\n",
    "| theZoo | Research | ~200 | 2010-2022 | APT/nation-state malware |\n",
    "| MalwareBazaar Recent | Wild | Variable | Last 30 days | Zero-day/emerging threats |\n",
    "| VirusShare (subset) | Wild | Variable | Mixed | Volume testing |\n",
    "| Benign (Sysinternals) | Legitimate | ~10 | Current | False positive baseline |\n",
    "| Benign (System32) | Legitimate | Variable | Current | System file testing |\n",
    "\n",
    "## Metrics Tracked\n",
    "- Accuracy, Precision, Recall, F1, AUC-ROC\n",
    "- Per-dataset breakdown\n",
    "- Confidence distribution analysis\n",
    "- Zero-day detection rate (samples < 30 days old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# EMBER for feature extraction\n",
    "import ember\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, roc_curve, classification_report\n",
    ")\n",
    "\n",
    "# Monkey patch for ember compatibility\n",
    "def fixed_section_info_process_raw_features(self, raw_obj):\n",
    "    sections = raw_obj['sections']\n",
    "    general = [\n",
    "        len(sections),\n",
    "        sum(1 for s in sections if s['size'] == 0),\n",
    "        sum(1 for s in sections if s['name'] == \"\"),\n",
    "        sum(1 for s in sections if 'MEM_READ' in s['props'] and 'MEM_EXECUTE' in s['props']),\n",
    "        sum(1 for s in sections if 'MEM_WRITE' in s['props'])\n",
    "    ]\n",
    "    section_sizes = [(s['name'], s['size']) for s in sections]\n",
    "    section_sizes_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_sizes]).toarray()[0]\n",
    "    section_entropy = [(s['name'], s['entropy']) for s in sections]\n",
    "    section_entropy_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_entropy]).toarray()[0]\n",
    "    section_vsize = [(s['name'], s['vsize']) for s in sections]\n",
    "    section_vsize_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_vsize]).toarray()[0]\n",
    "    entry_name_hashed = FeatureHasher(50, input_type=\"string\").transform([[raw_obj['entry']]]).toarray()[0]\n",
    "    characteristics = [p for s in sections for p in s['props'] if s['name'] == raw_obj['entry']]\n",
    "    characteristics_hashed = FeatureHasher(50, input_type=\"string\").transform([characteristics]).toarray()[0]\n",
    "    return np.hstack([general, section_sizes_hashed, section_entropy_hashed, section_vsize_hashed, entry_name_hashed, characteristics_hashed]).astype(np.float32)\n",
    "\n",
    "if hasattr(ember.features, 'SectionInfo'):\n",
    "    ember.features.SectionInfo.process_raw_features = fixed_section_info_process_raw_features\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "print(\"Evaluation framework loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== CONFIGURATION ==============\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path(\".\").resolve().parent\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "SAMPLES_DIR = PROJECT_ROOT / \"evaluation_samples\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"evaluation_results\"\n",
    "\n",
    "# Create directories\n",
    "SAMPLES_DIR.mkdir(exist_ok=True)\n",
    "(SAMPLES_DIR / \"malware\" / \"thezoo\").mkdir(parents=True, exist_ok=True)\n",
    "(SAMPLES_DIR / \"malware\" / \"malwarebazaar_recent\").mkdir(parents=True, exist_ok=True)\n",
    "(SAMPLES_DIR / \"malware\" / \"virusshare\").mkdir(parents=True, exist_ok=True)\n",
    "(SAMPLES_DIR / \"benign\" / \"sysinternals\").mkdir(parents=True, exist_ok=True)\n",
    "(SAMPLES_DIR / \"benign\" / \"system\").mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Classification threshold\n",
    "THRESHOLD = 0.35\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Models dir: {MODELS_DIR}\")\n",
    "print(f\"Samples dir: {SAMPLES_DIR}\")\n",
    "print(f\"Results dir: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataclasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SampleResult:\n",
    "    \"\"\"Result for a single sample evaluation.\"\"\"\n",
    "    filepath: str\n",
    "    sha256: str\n",
    "    dataset: str\n",
    "    expected_label: str\n",
    "    predicted_label: str\n",
    "    confidence: float\n",
    "    correct: bool\n",
    "    error: Optional[str] = None\n",
    "    sample_age_days: Optional[int] = None  # For zero-day analysis\n",
    "\n",
    "@dataclass \n",
    "class DatasetMetrics:\n",
    "    \"\"\"Aggregated metrics for a dataset.\"\"\"\n",
    "    name: str\n",
    "    total_samples: int\n",
    "    processed: int\n",
    "    errors: int\n",
    "    accuracy: float\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1: float\n",
    "    true_positives: int\n",
    "    false_positives: int\n",
    "    true_negatives: int\n",
    "    false_negatives: int\n",
    "    avg_confidence_malware: float\n",
    "    avg_confidence_benign: float\n",
    "\n",
    "@dataclass\n",
    "class EvaluationReport:\n",
    "    \"\"\"Complete evaluation report.\"\"\"\n",
    "    model_name: str\n",
    "    model_path: str\n",
    "    evaluation_date: str\n",
    "    threshold: float\n",
    "    overall_metrics: Dict\n",
    "    dataset_metrics: List[DatasetMetrics]\n",
    "    zero_day_metrics: Optional[Dict] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_loader",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"Evaluates malware detection models across multiple datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: Path, scaler_path: Path, pca_path: Optional[Path] = None):\n",
    "        \"\"\"Load model, scaler, and optionally PCA.\"\"\"\n",
    "        self.model = joblib.load(model_path)\n",
    "        self.scaler = joblib.load(scaler_path)\n",
    "        self.pca = joblib.load(pca_path) if pca_path and pca_path.exists() else None\n",
    "        self.extractor = ember.PEFeatureExtractor(2)\n",
    "        self.model_name = model_path.stem\n",
    "        self.model_path = str(model_path)\n",
    "        \n",
    "        print(f\"Loaded model: {self.model_name}\")\n",
    "        print(f\"  - PCA: {'Yes' if self.pca else 'No (full features)'}\")\n",
    "    \n",
    "    def predict_file(self, file_bytes: bytes, threshold: float = 0.35) -> Tuple[str, float]:\n",
    "        \"\"\"Predict if file is malicious.\"\"\"\n",
    "        try:\n",
    "            features = np.array(self.extractor.feature_vector(file_bytes), dtype=np.float32)\n",
    "            features = features.reshape(1, -1)\n",
    "            features_scaled = self.scaler.transform(features)\n",
    "            \n",
    "            if self.pca:\n",
    "                features_final = self.pca.transform(features_scaled)\n",
    "            else:\n",
    "                features_final = features_scaled\n",
    "            \n",
    "            if hasattr(self.model, \"predict_proba\"):\n",
    "                probs = self.model.predict_proba(features_final)\n",
    "                malicious_prob = float(probs[0][1])\n",
    "            else:\n",
    "                malicious_prob = float(self.model.predict(features_final)[0])\n",
    "            \n",
    "            label = \"malicious\" if malicious_prob > threshold else \"benign\"\n",
    "            return label, malicious_prob\n",
    "        except Exception as e:\n",
    "            return \"error\", 0.0\n",
    "    \n",
    "    def evaluate_sample(self, filepath: Path, expected_label: str, dataset: str, \n",
    "                       threshold: float = 0.35, sample_date: Optional[datetime] = None) -> SampleResult:\n",
    "        \"\"\"Evaluate a single sample.\"\"\"\n",
    "        try:\n",
    "            file_bytes = filepath.read_bytes()\n",
    "            sha256 = hashlib.sha256(file_bytes).hexdigest()\n",
    "            predicted, confidence = self.predict_file(file_bytes, threshold)\n",
    "            \n",
    "            age_days = None\n",
    "            if sample_date:\n",
    "                age_days = (datetime.now() - sample_date).days\n",
    "            \n",
    "            return SampleResult(\n",
    "                filepath=str(filepath),\n",
    "                sha256=sha256,\n",
    "                dataset=dataset,\n",
    "                expected_label=expected_label,\n",
    "                predicted_label=predicted,\n",
    "                confidence=confidence,\n",
    "                correct=(predicted == expected_label),\n",
    "                sample_age_days=age_days\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return SampleResult(\n",
    "                filepath=str(filepath),\n",
    "                sha256=\"\",\n",
    "                dataset=dataset,\n",
    "                expected_label=expected_label,\n",
    "                predicted_label=\"error\",\n",
    "                confidence=0.0,\n",
    "                correct=False,\n",
    "                error=str(e)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample_downloaders",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== SAMPLE DOWNLOADERS ==============\n",
    "\n",
    "def download_sysinternals(output_dir: Path, tools: List[str] = None) -> int:\n",
    "    \"\"\"Download Microsoft Sysinternals tools as benign samples.\"\"\"\n",
    "    if tools is None:\n",
    "        tools = [\n",
    "            \"procexp.exe\", \"procmon.exe\", \"autoruns.exe\", \"tcpview.exe\",\n",
    "            \"pslist.exe\", \"listdlls.exe\", \"handle.exe\", \"Dbgview.exe\",\n",
    "            \"strings.exe\", \"du.exe\", \"accesschk.exe\", \"psexec.exe\",\n",
    "            \"logonsessions.exe\", \"psinfo.exe\", \"diskext.exe\"\n",
    "        ]\n",
    "    \n",
    "    downloaded = 0\n",
    "    for tool in tools:\n",
    "        filepath = output_dir / tool\n",
    "        if filepath.exists():\n",
    "            continue\n",
    "        try:\n",
    "            url = f\"https://live.sysinternals.com/{tool}\"\n",
    "            response = requests.get(url, timeout=30)\n",
    "            if response.status_code == 200 and len(response.content) > 1000:\n",
    "                filepath.write_bytes(response.content)\n",
    "                downloaded += 1\n",
    "        except:\n",
    "            pass\n",
    "    return downloaded\n",
    "\n",
    "\n",
    "def download_malwarebazaar_recent(output_dir: Path, limit: int = 50) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Download recent malware samples from MalwareBazaar.\n",
    "    Returns metadata including first_seen date for zero-day analysis.\n",
    "    \"\"\"\n",
    "    metadata = []\n",
    "    \n",
    "    # Query for recent PE samples\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://mb-api.abuse.ch/api/v1/\",\n",
    "            data={\"query\": \"get_file_type\", \"file_type\": \"exe\", \"limit\": limit},\n",
    "            timeout=30\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            print(f\"MalwareBazaar API error: {response.status_code}\")\n",
    "            return metadata\n",
    "        \n",
    "        data = response.json()\n",
    "        if data.get(\"query_status\") != \"ok\":\n",
    "            print(f\"Query failed: {data.get('query_status')}\")\n",
    "            return metadata\n",
    "        \n",
    "        samples = data.get(\"data\", [])\n",
    "        print(f\"Found {len(samples)} recent samples from MalwareBazaar\")\n",
    "        \n",
    "        for sample in samples[:limit]:\n",
    "            sha256 = sample.get(\"sha256_hash\")\n",
    "            first_seen = sample.get(\"first_seen\", \"\")\n",
    "            \n",
    "            if not sha256:\n",
    "                continue\n",
    "            \n",
    "            filepath = output_dir / f\"{sha256[:16]}.exe\"\n",
    "            if filepath.exists():\n",
    "                metadata.append({\n",
    "                    \"sha256\": sha256,\n",
    "                    \"filepath\": str(filepath),\n",
    "                    \"first_seen\": first_seen\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Download sample\n",
    "            try:\n",
    "                dl_response = requests.post(\n",
    "                    \"https://mb-api.abuse.ch/api/v1/\",\n",
    "                    data={\"query\": \"get_file\", \"sha256_hash\": sha256},\n",
    "                    timeout=60\n",
    "                )\n",
    "                if dl_response.status_code == 200 and len(dl_response.content) > 100:\n",
    "                    # Extract from password-protected zip\n",
    "                    try:\n",
    "                        with zipfile.ZipFile(io.BytesIO(dl_response.content)) as zf:\n",
    "                            for name in zf.namelist():\n",
    "                                content = zf.read(name, pwd=b\"infected\")\n",
    "                                filepath.write_bytes(content)\n",
    "                                metadata.append({\n",
    "                                    \"sha256\": sha256,\n",
    "                                    \"filepath\": str(filepath),\n",
    "                                    \"first_seen\": first_seen\n",
    "                                })\n",
    "                                break\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading from MalwareBazaar: {e}\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "def collect_existing_samples(samples_dir: Path) -> Dict[str, List[Path]]:\n",
    "    \"\"\"Collect all existing samples organized by dataset.\"\"\"\n",
    "    samples = {\n",
    "        \"thezoo\": [],\n",
    "        \"malwarebazaar_recent\": [],\n",
    "        \"virusshare\": [],\n",
    "        \"sysinternals\": [],\n",
    "        \"system\": [],\n",
    "        \"other_malware\": [],\n",
    "        \"other_benign\": []\n",
    "    }\n",
    "    \n",
    "    # Malware samples\n",
    "    malware_dir = samples_dir / \"malware\"\n",
    "    if malware_dir.exists():\n",
    "        for subdir in malware_dir.iterdir():\n",
    "            if subdir.is_dir():\n",
    "                key = subdir.name if subdir.name in samples else \"other_malware\"\n",
    "                samples[key].extend([f for f in subdir.iterdir() if f.is_file()])\n",
    "    \n",
    "    # Benign samples  \n",
    "    benign_dir = samples_dir / \"benign\"\n",
    "    if benign_dir.exists():\n",
    "        for subdir in benign_dir.iterdir():\n",
    "            if subdir.is_dir():\n",
    "                key = subdir.name if subdir.name in samples else \"other_benign\"\n",
    "                samples[key].extend([f for f in subdir.iterdir() if f.is_file()])\n",
    "    \n",
    "    return samples\n",
    "\n",
    "\n",
    "print(\"Sample downloaders defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation_engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(results: List[SampleResult], dataset_name: str) -> DatasetMetrics:\n",
    "    \"\"\"Calculate metrics for a set of results.\"\"\"\n",
    "    valid_results = [r for r in results if r.error is None]\n",
    "    \n",
    "    if not valid_results:\n",
    "        return DatasetMetrics(\n",
    "            name=dataset_name, total_samples=len(results), processed=0, errors=len(results),\n",
    "            accuracy=0, precision=0, recall=0, f1=0,\n",
    "            true_positives=0, false_positives=0, true_negatives=0, false_negatives=0,\n",
    "            avg_confidence_malware=0, avg_confidence_benign=0\n",
    "        )\n",
    "    \n",
    "    y_true = [1 if r.expected_label == \"malicious\" else 0 for r in valid_results]\n",
    "    y_pred = [1 if r.predicted_label == \"malicious\" else 0 for r in valid_results]\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if len(set(y_true)) == 1 or len(set(y_pred)) == 1:\n",
    "        accuracy = sum(1 for r in valid_results if r.correct) / len(valid_results)\n",
    "        tp = sum(1 for r in valid_results if r.expected_label == \"malicious\" and r.predicted_label == \"malicious\")\n",
    "        fp = sum(1 for r in valid_results if r.expected_label == \"benign\" and r.predicted_label == \"malicious\")\n",
    "        tn = sum(1 for r in valid_results if r.expected_label == \"benign\" and r.predicted_label == \"benign\")\n",
    "        fn = sum(1 for r in valid_results if r.expected_label == \"malicious\" and r.predicted_label == \"benign\")\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    else:\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    \n",
    "    # Confidence analysis\n",
    "    mal_confs = [r.confidence for r in valid_results if r.expected_label == \"malicious\"]\n",
    "    ben_confs = [r.confidence for r in valid_results if r.expected_label == \"benign\"]\n",
    "    \n",
    "    return DatasetMetrics(\n",
    "        name=dataset_name,\n",
    "        total_samples=len(results),\n",
    "        processed=len(valid_results),\n",
    "        errors=len(results) - len(valid_results),\n",
    "        accuracy=accuracy,\n",
    "        precision=precision,\n",
    "        recall=recall,\n",
    "        f1=f1,\n",
    "        true_positives=int(tp) if 'tp' in dir() else 0,\n",
    "        false_positives=int(fp) if 'fp' in dir() else 0,\n",
    "        true_negatives=int(tn) if 'tn' in dir() else 0,\n",
    "        false_negatives=int(fn) if 'fn' in dir() else 0,\n",
    "        avg_confidence_malware=np.mean(mal_confs) if mal_confs else 0,\n",
    "        avg_confidence_benign=np.mean(ben_confs) if ben_confs else 0\n",
    "    )\n",
    "\n",
    "\n",
    "def run_evaluation(evaluator: ModelEvaluator, samples: Dict[str, List[Path]], \n",
    "                   threshold: float = 0.35, max_workers: int = 4) -> Tuple[List[SampleResult], List[DatasetMetrics]]:\n",
    "    \"\"\"Run full evaluation across all datasets.\"\"\"\n",
    "    all_results = []\n",
    "    dataset_metrics = []\n",
    "    \n",
    "    # Define which datasets are malware vs benign\n",
    "    malware_datasets = [\"thezoo\", \"malwarebazaar_recent\", \"virusshare\", \"other_malware\"]\n",
    "    benign_datasets = [\"sysinternals\", \"system\", \"other_benign\"]\n",
    "    \n",
    "    for dataset_name, files in samples.items():\n",
    "        if not files:\n",
    "            continue\n",
    "        \n",
    "        expected_label = \"malicious\" if dataset_name in malware_datasets else \"benign\"\n",
    "        print(f\"\\nEvaluating {dataset_name} ({len(files)} samples, expected: {expected_label})...\")\n",
    "        \n",
    "        results = []\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {\n",
    "                executor.submit(evaluator.evaluate_sample, f, expected_label, dataset_name, threshold): f\n",
    "                for f in files\n",
    "            }\n",
    "            \n",
    "            for i, future in enumerate(as_completed(futures)):\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                if (i + 1) % 20 == 0:\n",
    "                    print(f\"  Processed {i + 1}/{len(files)}...\")\n",
    "        \n",
    "        all_results.extend(results)\n",
    "        metrics = calculate_metrics(results, dataset_name)\n",
    "        dataset_metrics.append(metrics)\n",
    "        \n",
    "        print(f\"  Accuracy: {metrics.accuracy:.1%} | Recall: {metrics.recall:.1%} | Precision: {metrics.precision:.1%}\")\n",
    "    \n",
    "    return all_results, dataset_metrics\n",
    "\n",
    "\n",
    "print(\"Evaluation engine defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation_results(results: List[SampleResult], metrics: List[DatasetMetrics], \n",
    "                           model_name: str, output_dir: Path):\n",
    "    \"\"\"Generate visualization plots for evaluation results.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(f\"Evaluation Results: {model_name}\", fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 1. Accuracy by dataset\n",
    "    ax = axes[0, 0]\n",
    "    names = [m.name for m in metrics if m.processed > 0]\n",
    "    accuracies = [m.accuracy for m in metrics if m.processed > 0]\n",
    "    colors = ['#2ecc71' if a >= 0.8 else '#f39c12' if a >= 0.6 else '#e74c3c' for a in accuracies]\n",
    "    bars = ax.barh(names, accuracies, color=colors)\n",
    "    ax.set_xlabel('Accuracy')\n",
    "    ax.set_title('Accuracy by Dataset')\n",
    "    ax.set_xlim(0, 1)\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        ax.text(acc + 0.02, bar.get_y() + bar.get_height()/2, f'{acc:.1%}', va='center')\n",
    "    \n",
    "    # 2. Precision vs Recall\n",
    "    ax = axes[0, 1]\n",
    "    for m in metrics:\n",
    "        if m.processed > 0:\n",
    "            ax.scatter(m.recall, m.precision, s=m.processed*2, alpha=0.7, label=m.name)\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_title('Precision vs Recall by Dataset')\n",
    "    ax.set_xlim(0, 1.05)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.legend(loc='lower left', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Confidence distribution\n",
    "    ax = axes[0, 2]\n",
    "    malware_confs = [r.confidence for r in results if r.expected_label == \"malicious\" and r.error is None]\n",
    "    benign_confs = [r.confidence for r in results if r.expected_label == \"benign\" and r.error is None]\n",
    "    if malware_confs:\n",
    "        ax.hist(malware_confs, bins=30, alpha=0.7, label=f'Malware (n={len(malware_confs)})', color='red')\n",
    "    if benign_confs:\n",
    "        ax.hist(benign_confs, bins=30, alpha=0.7, label=f'Benign (n={len(benign_confs)})', color='green')\n",
    "    ax.axvline(x=0.35, color='black', linestyle='--', label='Threshold (0.35)')\n",
    "    ax.set_xlabel('Confidence (P(Malicious))')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Confidence Distribution')\n",
    "    ax.legend()\n",
    "    \n",
    "    # 4. Confusion matrix (overall)\n",
    "    ax = axes[1, 0]\n",
    "    valid_results = [r for r in results if r.error is None]\n",
    "    y_true = [1 if r.expected_label == \"malicious\" else 0 for r in valid_results]\n",
    "    y_pred = [1 if r.predicted_label == \"malicious\" else 0 for r in valid_results]\n",
    "    if len(set(y_true)) > 1 and len(set(y_pred)) > 1:\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                    xticklabels=['Benign', 'Malicious'], yticklabels=['Benign', 'Malicious'])\n",
    "    ax.set_title('Overall Confusion Matrix')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    \n",
    "    # 5. Sample counts by dataset\n",
    "    ax = axes[1, 1]\n",
    "    names = [m.name for m in metrics]\n",
    "    counts = [m.processed for m in metrics]\n",
    "    errors = [m.errors for m in metrics]\n",
    "    x = np.arange(len(names))\n",
    "    ax.bar(x, counts, label='Processed', color='steelblue')\n",
    "    ax.bar(x, errors, bottom=counts, label='Errors', color='salmon')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(names, rotation=45, ha='right')\n",
    "    ax.set_ylabel('Sample Count')\n",
    "    ax.set_title('Samples by Dataset')\n",
    "    ax.legend()\n",
    "    \n",
    "    # 6. F1 scores\n",
    "    ax = axes[1, 2]\n",
    "    f1_scores = [m.f1 for m in metrics if m.processed > 0]\n",
    "    names = [m.name for m in metrics if m.processed > 0]\n",
    "    colors = ['#2ecc71' if f >= 0.8 else '#f39c12' if f >= 0.6 else '#e74c3c' for f in f1_scores]\n",
    "    bars = ax.barh(names, f1_scores, color=colors)\n",
    "    ax.set_xlabel('F1 Score')\n",
    "    ax.set_title('F1 Score by Dataset')\n",
    "    ax.set_xlim(0, 1)\n",
    "    for bar, f1 in zip(bars, f1_scores):\n",
    "        ax.text(f1 + 0.02, bar.get_y() + bar.get_height()/2, f'{f1:.2f}', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / f\"evaluation_{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Visualization functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "report_generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(results: List[SampleResult], metrics: List[DatasetMetrics], \n",
    "                   evaluator: ModelEvaluator, threshold: float, output_dir: Path) -> EvaluationReport:\n",
    "    \"\"\"Generate and save comprehensive evaluation report.\"\"\"\n",
    "    \n",
    "    # Overall metrics\n",
    "    valid_results = [r for r in results if r.error is None]\n",
    "    overall_correct = sum(1 for r in valid_results if r.correct)\n",
    "    \n",
    "    overall = {\n",
    "        \"total_samples\": len(results),\n",
    "        \"processed\": len(valid_results),\n",
    "        \"errors\": len(results) - len(valid_results),\n",
    "        \"accuracy\": overall_correct / len(valid_results) if valid_results else 0,\n",
    "        \"total_malware_samples\": sum(1 for r in valid_results if r.expected_label == \"malicious\"),\n",
    "        \"total_benign_samples\": sum(1 for r in valid_results if r.expected_label == \"benign\"),\n",
    "    }\n",
    "    \n",
    "    # Zero-day analysis (samples seen in last 30 days)\n",
    "    zero_day_results = [r for r in valid_results if r.sample_age_days is not None and r.sample_age_days <= 30]\n",
    "    zero_day_metrics = None\n",
    "    if zero_day_results:\n",
    "        zd_correct = sum(1 for r in zero_day_results if r.correct)\n",
    "        zero_day_metrics = {\n",
    "            \"samples\": len(zero_day_results),\n",
    "            \"accuracy\": zd_correct / len(zero_day_results),\n",
    "            \"avg_age_days\": np.mean([r.sample_age_days for r in zero_day_results])\n",
    "        }\n",
    "    \n",
    "    report = EvaluationReport(\n",
    "        model_name=evaluator.model_name,\n",
    "        model_path=evaluator.model_path,\n",
    "        evaluation_date=datetime.now().isoformat(),\n",
    "        threshold=threshold,\n",
    "        overall_metrics=overall,\n",
    "        dataset_metrics=metrics,\n",
    "        zero_day_metrics=zero_day_metrics\n",
    "    )\n",
    "    \n",
    "    # Save report as JSON\n",
    "    report_path = output_dir / f\"report_{evaluator.model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    \n",
    "    report_dict = {\n",
    "        \"model_name\": report.model_name,\n",
    "        \"model_path\": report.model_path,\n",
    "        \"evaluation_date\": report.evaluation_date,\n",
    "        \"threshold\": report.threshold,\n",
    "        \"overall_metrics\": report.overall_metrics,\n",
    "        \"dataset_metrics\": [asdict(m) for m in report.dataset_metrics],\n",
    "        \"zero_day_metrics\": report.zero_day_metrics\n",
    "    }\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(report_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nReport saved to: {report_path}\")\n",
    "    \n",
    "    # Save detailed results CSV\n",
    "    results_df = pd.DataFrame([asdict(r) for r in results])\n",
    "    csv_path = output_dir / f\"details_{evaluator.model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Detailed results saved to: {csv_path}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "\n",
    "def print_summary(report: EvaluationReport):\n",
    "    \"\"\"Print formatted summary of evaluation.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"EVALUATION SUMMARY: {report.model_name}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Date: {report.evaluation_date}\")\n",
    "    print(f\"Threshold: {report.threshold}\")\n",
    "    print(f\"\\nOverall: {report.overall_metrics['accuracy']:.1%} accuracy \")\n",
    "    print(f\"         ({report.overall_metrics['processed']} samples, {report.overall_metrics['errors']} errors)\")\n",
    "    \n",
    "    print(\"\\nPer-Dataset Results:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Dataset':<25} {'Samples':>8} {'Accuracy':>10} {'Precision':>10} {'Recall':>10} {'F1':>8}\")\n",
    "    print(\"-\" * 70)\n",
    "    for m in report.dataset_metrics:\n",
    "        if m.processed > 0:\n",
    "            print(f\"{m.name:<25} {m.processed:>8} {m.accuracy:>10.1%} {m.precision:>10.1%} {m.recall:>10.1%} {m.f1:>8.2f}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    if report.zero_day_metrics:\n",
    "        print(f\"\\nZero-Day Detection (samples < 30 days old):\")\n",
    "        print(f\"  Samples: {report.zero_day_metrics['samples']}\")\n",
    "        print(f\"  Accuracy: {report.zero_day_metrics['accuracy']:.1%}\")\n",
    "        print(f\"  Avg Age: {report.zero_day_metrics['avg_age_days']:.1f} days\")\n",
    "\n",
    "\n",
    "print(\"Report generator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_eval_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Evaluation\n",
    "\n",
    "Execute the cells below to run a full evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== STEP 1: Download/Collect Samples ==============\n",
    "\n",
    "print(\"Setting up evaluation samples...\\n\")\n",
    "\n",
    "# Download Sysinternals (benign)\n",
    "print(\"Downloading Sysinternals tools...\")\n",
    "n_sysinternals = download_sysinternals(SAMPLES_DIR / \"benign\" / \"sysinternals\")\n",
    "print(f\"  Downloaded {n_sysinternals} new tools\")\n",
    "\n",
    "# Copy existing malware samples if they exist\n",
    "existing_malware = PROJECT_ROOT / \"malware_samples\"\n",
    "if existing_malware.exists():\n",
    "    import shutil\n",
    "    dest = SAMPLES_DIR / \"malware\" / \"thezoo\"\n",
    "    for f in existing_malware.iterdir():\n",
    "        if f.is_file() and not (dest / f.name).exists():\n",
    "            shutil.copy(f, dest / f.name)\n",
    "    print(f\"Copied existing malware samples to thezoo directory\")\n",
    "\n",
    "# Copy existing benign samples\n",
    "existing_benign = PROJECT_ROOT / \"benign_samples\"\n",
    "if existing_benign.exists():\n",
    "    dest = SAMPLES_DIR / \"benign\" / \"sysinternals\"\n",
    "    for f in existing_benign.iterdir():\n",
    "        if f.is_file() and not (dest / f.name).exists():\n",
    "            shutil.copy(f, dest / f.name)\n",
    "    print(f\"Copied existing benign samples\")\n",
    "\n",
    "# Optional: Download recent MalwareBazaar samples (uncomment to enable)\n",
    "# print(\"\\nDownloading recent samples from MalwareBazaar...\")\n",
    "# mb_metadata = download_malwarebazaar_recent(SAMPLES_DIR / \"malware\" / \"malwarebazaar_recent\", limit=30)\n",
    "# print(f\"  Downloaded {len(mb_metadata)} recent samples\")\n",
    "\n",
    "# Collect all samples\n",
    "samples = collect_existing_samples(SAMPLES_DIR)\n",
    "\n",
    "print(\"\\nSample counts:\")\n",
    "for name, files in samples.items():\n",
    "    if files:\n",
    "        print(f\"  {name}: {len(files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== STEP 2: Load Model ==============\n",
    "\n",
    "# Choose which model to evaluate:\n",
    "\n",
    "# Option A: Improved model (no PCA, 97.5% EMBER accuracy)\n",
    "evaluator = ModelEvaluator(\n",
    "    model_path=MODELS_DIR / \"model_improved.pkl\",\n",
    "    scaler_path=MODELS_DIR / \"scaler_improved.pkl\",\n",
    "    pca_path=None  # No PCA for improved model\n",
    ")\n",
    "\n",
    "# Option B: Original model (with PCA, 93% EMBER accuracy)\n",
    "# evaluator = ModelEvaluator(\n",
    "#     model_path=MODELS_DIR / \"xgboost_pca_model.pkl\",\n",
    "#     scaler_path=MODELS_DIR / \"scaler.pkl\",\n",
    "#     pca_path=MODELS_DIR / \"pca_transform.pkl\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== STEP 3: Run Evaluation ==============\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "start_time = time.time()\n",
    "\n",
    "results, metrics = run_evaluation(\n",
    "    evaluator=evaluator,\n",
    "    samples=samples,\n",
    "    threshold=THRESHOLD,\n",
    "    max_workers=4\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nEvaluation completed in {elapsed:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_outputs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== STEP 4: Generate Report & Visualizations ==============\n",
    "\n",
    "# Generate report\n",
    "report = generate_report(results, metrics, evaluator, THRESHOLD, RESULTS_DIR)\n",
    "\n",
    "# Print summary\n",
    "print_summary(report)\n",
    "\n",
    "# Generate plots\n",
    "plot_evaluation_results(results, metrics, evaluator.model_name, RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Compare Multiple Models\n",
    "\n",
    "Use this section to compare different model versions side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model_configs: List[Dict], samples: Dict[str, List[Path]], \n",
    "                   threshold: float = 0.35) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare multiple models on the same sample set.\n",
    "    \n",
    "    model_configs: List of dicts with keys: name, model_path, scaler_path, pca_path (optional)\n",
    "    \"\"\"\n",
    "    comparison_data = []\n",
    "    \n",
    "    for config in model_configs:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Evaluating: {config['name']}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        evaluator = ModelEvaluator(\n",
    "            model_path=Path(config['model_path']),\n",
    "            scaler_path=Path(config['scaler_path']),\n",
    "            pca_path=Path(config['pca_path']) if config.get('pca_path') else None\n",
    "        )\n",
    "        \n",
    "        results, metrics = run_evaluation(evaluator, samples, threshold, max_workers=4)\n",
    "        \n",
    "        # Overall stats\n",
    "        valid = [r for r in results if r.error is None]\n",
    "        overall_acc = sum(1 for r in valid if r.correct) / len(valid) if valid else 0\n",
    "        \n",
    "        row = {\n",
    "            'Model': config['name'],\n",
    "            'Overall Accuracy': overall_acc,\n",
    "            'Total Samples': len(valid)\n",
    "        }\n",
    "        \n",
    "        for m in metrics:\n",
    "            if m.processed > 0:\n",
    "                row[f'{m.name}_acc'] = m.accuracy\n",
    "                row[f'{m.name}_f1'] = m.f1\n",
    "        \n",
    "        comparison_data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Example usage (uncomment to compare models):\n",
    "# model_configs = [\n",
    "#     {\n",
    "#         \"name\": \"v1_pca\",\n",
    "#         \"model_path\": MODELS_DIR / \"xgboost_pca_model.pkl\",\n",
    "#         \"scaler_path\": MODELS_DIR / \"scaler.pkl\",\n",
    "#         \"pca_path\": MODELS_DIR / \"pca_transform.pkl\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"v2_improved\",\n",
    "#         \"model_path\": MODELS_DIR / \"model_improved.pkl\",\n",
    "#         \"scaler_path\": MODELS_DIR / \"scaler_improved.pkl\",\n",
    "#         \"pca_path\": None\n",
    "#     }\n",
    "# ]\n",
    "# comparison_df = compare_models(model_configs, samples)\n",
    "# display(comparison_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
